<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>注意力机制、自注意力机制和Transformer系统学习 | Daydreamer-Li&#39;s blog</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://daydreamer-li.github.io//favicon.ico?v=1664356335856">
<link rel="stylesheet" href="https://daydreamer-li.github.io//styles/main.css">


  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css" />
  

  


<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="关于注意力机制、自注意力机制和Transformer的学习

Attention机制（2014年提出）
相关论文：Bahdanau D, Cho K, Bengio Y. Neural machine translation by join..." />
    <meta name="keywords" content="" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://daydreamer-li.github.io/">
        <img src="https://daydreamer-li.github.io//images/avatar.png?v=1664356335856" class="site-logo">
        <h1 class="site-title">Daydreamer-Li&#39;s blog</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      路漫漫其修远兮，吾将上下而求索。
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://daydreamer-li.github.io//atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">注意力机制、自注意力机制和Transformer系统学习</h2>
            <div class="post-date">2022-07-27</div>
            
            <div class="post-content" v-pre>
              <p>关于注意力机制、自注意力机制和Transformer的学习</p>
<!-- more -->
<h1 id="attention机制2014年提出">Attention机制（2014年提出）</h1>
<p>相关论文：Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014</p>
<h2 id="1-引入attetion机制的原因">1. 引入attetion机制的原因</h2>
<p>（1）计算能力限制：信息越多，模型越复杂，计算能力限制神经网络的发展<br>
（2）优化算法限制：LSTM只能在一定程度上缓解RNN中的长距离依赖问题，且信息“记忆”能力并不高。<br>
（3） 缺点：需要的数据量大，当数据量少时，注意力机制效果不如BiLstm和LSTM</p>
<h2 id="2-简要理解和过程">2. 简要理解和过程</h2>
<p>（1） 定性解释： Attention是从大量信息中有筛选出少量重要信息，并聚焦到这些重要信息上，忽略大多不重要的信息。权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。<br>
（2） 抽象计算过程：第一个过程是根据Query和Key计算权重系数（根据Query和Key计算两者的相似性或者相关性），第二个过程根据权重系数对Value进行加权求和。</p>
<figure data-type="image" tabindex="1"><img src="https://daydreamer-li.github.io//post-images/1658976639132.jpg" alt="" title="attention计算示意图" loading="lazy"></figure>
<h2 id="3-具体过程">3.  具体过程：</h2>
<ul>
<li>第一阶段音符不同的函数和计算机制，根据Query和某一个keyi，计算两者相似性或者相关性，常见方法有：求两者的向量点积、求两者的向量Cosine相似性或者在引入额外的神经网络（例如MLP）求值；</li>
<li>第二阶段通过类似Softmax的方法对第一阶段得到的相关性值归一化结果，将原始的相关值通过函数转换为0-1的概率分布；</li>
<li>最后通过加权求和得到Attention的数值，具体公式如下，其中ai是第二阶段计算的概率值，为valuei对应的权重系数<br>
<img src="https://daydreamer-li.github.io//post-images/1658977491596.png" alt="" title="attention数值计算公式" loading="lazy"></li>
</ul>
<h1 id="self-attention机制">Self-attention机制</h1>
<p>原文链接：<a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p>
<h2 id="1-提升点和优缺点">1. 提升点和优缺点</h2>
<p>（1）相较于注意力机制而言，减少了对外部信息的依赖，更擅长捕捉数据或特征的内部相关性<br>
（2）优点<br>
- 参数少：相比于 CNN、RNN ，其复杂度更小，参数也更少。所以对算力的要求也就更小。<br>
- 速度快：Attention 解决了 RNN及其变体模型 不能并行计算的问题。Attention机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理。<br>
- 效果好：引入Attention 机制，可以学习长距离的信息<br>
（3）缺点同attention机制</p>
<h2 id="2-在文本中的计算过程通过计算单词间的相互影响来解决长距离依赖问题">2. 在文本中的计算过程（通过计算单词间的相互影响，来解决长距离依赖问题）</h2>
<p>（1） 将输入单词转化成嵌入向量；<br>
（2）根据嵌入向量得到q，k，v三个向量；<br>
（3）为每个向量计算一个score：score =q . k ；<br>
（4）为了梯度的稳定，Transformer使用了score归一化，即除以  ；<br>
（5）对score施以softmax激活函数；<br>
（6）softmax点乘Value值v，得到加权的每个输入向量的评分v；<br>
（7） 相加之后得到最终的输出结果z ：对 v求和。</p>
<h2 id="3-详细的self-attention处理过程">3. 详细的self-attention处理过程</h2>
<p>（1）首先，self-attention会计算出三个新的向量，在论文中，向量的维度是<strong>512维</strong>，我们把这三个向量分别称为Query、Key、Value，这三个向量是用embedding向量与一个矩阵相乘得到的结果，这个矩阵是随机初始化的，维度为（64，512）注意第二个维度需要和embedding的维度一样，其值在BP的过程中会一直进行更新，得到的这三个向量的维度是64低于embedding维度的<br>
<img src="https://daydreamer-li.github.io//post-images/1658991135377.png" alt="" loading="lazy"></p>
<p>接下来阐述Query、Key和value的具体作用：<br>
- 计算self-attention的分数值，该分数值决定了当我们在某个位置encode一个词时，对输入句子的其他部分的关注程度。这个分数值的计算方法是Query与Key做点乘，以下图为例，首先我们需要针对Thinking这个词，计算出其他词对于该词的一个分数值，首先是针对于自己本身即q1·k1，然后是针对于第二个词即q1·k2<br>
<img src="https://daydreamer-li.github.io//post-images/1658991687173.png" alt="" title="计算self-attention的分数值" loading="lazy"></p>
<ul>
<li>
<p>接下来，把点乘的结果除以一个常数，这里我们除以8，这个值一般是采用上文提到的矩阵的第一个维度的开方即64的开方8，当然也可以选择其他的值，然后把得到的结果做一个softmax的计算。得到的结果即是每个词对于当前位置的词的相关性大小。<br>
<img src="https://daydreamer-li.github.io//post-images/1658992021226.png" alt="" loading="lazy"></p>
<ul>
<li>下一步就是把Value和softmax得到的值进行相乘，并相加，得到的结果即是self-attetion在当前节点的值<a name="self-attention计算图"></a><br>
<img src="https://daydreamer-li.github.io//post-images/1658992081871.png" alt="" loading="lazy"><br>
<img src="https://daydreamer-li.github.io//post-images/1658992143076.png" alt="" loading="lazy"></li>
</ul>
</li>
</ul>
<p>这种通过query和key的相似程度来确定value的权重分布的方法被称为scaled dot-product attention<img src="https://daydreamer-li.github.io//post-images/1658993118948.png" alt="" title="矩阵计算公式" loading="lazy"></p>
<h1 id="transformer">Transformer</h1>
<p>** 总体概括：Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。更准确地讲，Transformer由且仅由self-Attenion和Feed Forward Neural Network组成。**<br>
原文链接：<a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a><br>
知乎链接：<a href="https://zhuanlan.zhihu.com/p/48508221">https://zhuanlan.zhihu.com/p/48508221</a><br>
##　1. 优点：<br>
（1）虽然Transformer最终也没有逃脱传统学习的套路，Transformer也只是一个全连接（或者是一维卷积）加Attention的结合体。但是其设计已经足够有创新，因为其抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩，值得每个深度学习的相关人员仔细研究和品位。（2）Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离是1，这对解决NLP中棘手的长期依赖问题是非常有效的。<br>
（3）Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向。<br>
（4）算法的并行性非常好，符合目前的硬件（主要指GPU）环境。</p>
<p>##　2. 缺点：<br>
（1）粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型丧失了捕捉局部特征的能力，RNN + CNN + Transformer的结合可能会带来更好的效果。<br>
（2）Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。</p>
<p>##　3. 高层Transformer简单结构<br>
Transformer本质上是一个Encoder-Decoder的结构，举例来说，当编码器由6个编码bloack组成，同样解码器是6个解码block组成，编码器的输出会作为所有解码block的输入。结构示意图如下：</p>
<figure data-type="image" tabindex="2"><img src="https://daydreamer-li.github.io//post-images/1659009668701.png" alt="" title="Encoder和Decoder的结构示意图" loading="lazy"></figure>
<ul>
<li>在Encoder中，数据首先会经过一个叫做‘self-attention’的模块得到一个加权之后的特征向量称为Z，具体公式见上文self-attention中最后一张图片；得到Z后，会被送到encoder的下一个模块，即Feed Forward NeuralNetwork，这个全连接层有两层，第一层的激活函数是ReLU，第二层是一个线性激活函数，可以表示为：<br>
<img src="https://daydreamer-li.github.io//post-images/1659010093449.png" alt="" title="Feed Forward Neural Network" loading="lazy"></li>
<li>Decoder的结构，它和encoder的不同之处在于Decoder多了一个Encoder-Decoder Attention，两个Attention分别用于计算输入和输出的权值：<br>
Self-Attention：当前翻译和已经翻译的前文之间的关系；<br>
Encoder-Decnoder Attention：当前翻译和编码的特征向量之间的关系。</li>
</ul>
<p>在self-attention中最后一点是采用了残差网络中的short-cut结构，解决深度学习中的退化问题，结果如下图所示:<br>
<img src="https://daydreamer-li.github.io//post-images/1658995016644.png" alt="" title="Self-attention中的short-cut连接" loading="lazy"></p>
<p>##　4. multi-head Attention<br>
Multi-Head Attention相当于<em>ｈ</em>个不同的self-attention的集成（ensemble），在这里我们以  <em>ｈ＝８</em>　举例说明。Multi-Head Attention的输出分成3步：<br>
<img src="https://daydreamer-li.github.io//post-images/1659010935776.png" alt="" loading="lazy"><br>
整个过程如下图，同self-attention一样，multi-head attention也加入了short-cut机制。<br>
<img src="https://daydreamer-li.github.io//post-images/1659010940540.png" alt="" loading="lazy"></p>
<h2 id="5encoder-decoder-attention">5.Encoder-Decoder Attention</h2>
<p>在解码器中，Transformer block比编码器中多了个encoder-cecoder attention。在encoder-decoder attention中，<em>Q</em>来自于解码器的上一个输出,<em>K</em>和<em>V</em>则来自于与编码器的输出。其计算方式完全和<a href="#self-attention%E8%AE%A1%E7%AE%97%E5%9B%BE">该图</a> 的过程相同。</p>
<p>由于在机器翻译中，解码过程是一个顺序操作的过程，也就是当解码第 <em>k</em>个特征向量时，我们只能看到第<em>k-1</em>及其之前的解码结果，论文中把这种情况下的multi-head attention叫做masked multi-head attention。</p>
<h2 id="6损失层">6.损失层</h2>
<p>解码器解码之后，解码的特征向量经过一层激活函数为softmax的全连接层之后得到反映每个单词概率的输出向量。此时我们便可以通过CTC等损失函数训练模型了。</p>
<p>而一个完整可训练的网络结构便是encoder和decoder的堆叠（各<em>N</em>个，<em>N=6</em>），我们可以得到下图中的完整的Transformer的结构：<img src="https://daydreamer-li.github.io//post-images/1659011461553.png" alt="" loading="lazy"></p>
<h2 id="7位置编码">7.位置编码</h2>
<p>截止目前为止，我们介绍的Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子的结构怎么打乱，Transformer都会得到类似的结果。换句话说，Transformer只是一个功能更强大的词袋模型而已。</p>
<p>为了解决这个问题，论文中在编码词向量时引入了位置编码（Position Embedding）的特征。具体地说，位置编码会在词向量中加入了单词的位置信息，这样Transformer就能区分不同位置的单词了。</p>
<p>那么怎么编码这个位置信息呢？常见的模式有：a. 根据数据学习；b. 自己设计编码规则。在这里作者采用了第二种方式。那么这个位置编码该是什么样子呢？通常位置编码是一个长度为<em>dmodel</em>的特征向量，这样便于和词向量进行单位加的操作<br>
<img src="https://daydreamer-li.github.io//post-images/1659011529361.png" alt="" title="位置编码公式" loading="lazy"><br>
<img src="https://daydreamer-li.github.io//post-images/1659011550064.png" alt="" title="位置编码公式解释" loading="lazy"></p>

            </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://daydreamer-li.github.io/post/dataset_and_result/">
                  <h3 class="post-title">
                    合并数据集之后的结果记录
                  </h3>
                </a>
              </div>
            

            
              
                <div id="gitalk-container" data-aos="fade-in"></div>
              

              
            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>





  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>

      var gitalk = new Gitalk({
        clientID: 'db9099181f6c8d14f0a2',
        clientSecret: '170f41e431a314a357b4d2074cd94248f55d4b52',
        repo: 'Daydreamer-Li.github.io',
        owner: 'Daydreamer-Li',
        admin: ['Daydreamer-Li'],
        id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })

      gitalk.render('gitalk-container')

    </script>
  

  




  </body>
</html>
