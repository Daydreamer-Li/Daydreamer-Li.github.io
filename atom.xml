<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://daydreamer-li.github.io/</id>
    <title>Daydreamer-Li&apos;s blog</title>
    <updated>2022-09-28T09:12:18.044Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://daydreamer-li.github.io/"/>
    <link rel="self" href="https://daydreamer-li.github.io/atom.xml"/>
    <subtitle>路漫漫其修远兮，吾将上下而求索。</subtitle>
    <logo>https://daydreamer-li.github.io/images/avatar.png</logo>
    <icon>https://daydreamer-li.github.io/favicon.ico</icon>
    <rights>All rights reserved 2022, Daydreamer-Li&apos;s blog</rights>
    <entry>
        <title type="html"><![CDATA[音频与视频关键点初步融合结果记录]]></title>
        <id>https://daydreamer-li.github.io/post/audio_with_keypoints/</id>
        <link href="https://daydreamer-li.github.io/post/audio_with_keypoints/">
        </link>
        <updated>2022-09-22T06:53:29.000Z</updated>
        <summary type="html"><![CDATA[<p>记录纯音频模型和加上不同融合方法的转录结果</p>
]]></summary>
        <content type="html"><![CDATA[<p>记录纯音频模型和加上不同融合方法的转录结果</p>
<!-- more -->
<h2 id="基础音频转录">基础音频转录</h2>
<p>（训练集：OMAPS:80 测试集：OMAPS:26，验证集：OMAP,直接用在OMAPS预训练好的模型在OMAP上训练）<br>
onset阈值0.15 纯音频<br>
模型地址：&quot;/home/data/liyuqing/bysj_new/onsets-and-frames-master/onsets-and-frames-master/runs/use_for_test220916-100632/model-0.9103.pt&quot;</p>
<table>
<thead>
<tr>
<th>dataset</th>
<th style="text-align:center">F1-measure</th>
<th style="text-align:right">Precision</th>
<th style="text-align:right">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>OMAP</td>
<td style="text-align:center">0.8544</td>
<td style="text-align:right">0.9343</td>
<td style="text-align:right">0.7912</td>
</tr>
<tr>
<td>OMAPS</td>
<td style="text-align:center">0.9164</td>
<td style="text-align:right">0.9531</td>
<td style="text-align:right">0.8843</td>
</tr>
</tbody>
</table>
<h2 id="基础音频转录加入视频数据的分支">基础音频转录加入视频数据的分支</h2>
<p>（即在使用BiLTSM之前加入视频的标签提高Recall)<br>
onset阈值0.15 音频和关键点共同作用<br>
模型地址：<br>
&quot;/home/data/liyuqing/code_VAF/onsets-and-frames-master/runs/one_video_label-220830-143538/model-0.8963.pt&quot;</p>
<table>
<thead>
<tr>
<th>dataset</th>
<th style="text-align:center">F1-measure</th>
<th style="text-align:right">Precision</th>
<th style="text-align:right">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>OMAP</td>
<td style="text-align:center">0.8509</td>
<td style="text-align:right">0.8682</td>
<td style="text-align:right">0.8371</td>
</tr>
<tr>
<td>OMAPS</td>
<td style="text-align:center">0.9032</td>
<td style="text-align:right">0.9174</td>
<td style="text-align:right">0.8904</td>
</tr>
</tbody>
</table>
<p>可以看到视频信息对于解决漏检问题有一定帮助，但目前弊大于利，需要选择更合适的方式融合视频帧对应的信息。</p>
<h2 id="在基础音频转录加入视频数据的分支的基础上做后处理">在基础音频转录加入视频数据的分支的基础上做后处理</h2>
<p>（即将视频标签外的音符去除）</p>
<table>
<thead>
<tr>
<th>dataset</th>
<th style="text-align:center">F1-measure</th>
<th style="text-align:right">Precision</th>
<th style="text-align:right">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>OMAP</td>
<td style="text-align:center">0.8641</td>
<td style="text-align:right">0.9058</td>
<td style="text-align:right">0.8287</td>
</tr>
<tr>
<td>OMAPS</td>
<td style="text-align:center">0.9057</td>
<td style="text-align:right">0.9336</td>
<td style="text-align:right">0.8806</td>
</tr>
</tbody>
</table>
<p>OMAP相较于无后处理有一定提升，相较于纯音频只提升了1%<br>
OMAPS相较于纯音频不升反降，与无后处理的音视频初步融合结果基本持平</p>
<h2 id="阐明一下阈值选取为015的原因">阐明一下阈值选取为0.15的原因</h2>
<p>记录一下onset阈值为0.5的结果，在该阈值下的结果不符合实验预期，经过对比选择后最终Onset的阈值选取为0.15<br>
纯音频</p>
<table>
<thead>
<tr>
<th>dataset</th>
<th style="text-align:center">F1-measure</th>
<th style="text-align:right">Precision</th>
<th style="text-align:right">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>OMAP</td>
<td style="text-align:center">0.8242</td>
<td style="text-align:right">0.9577</td>
<td style="text-align:right">0.7292</td>
</tr>
<tr>
<td>OMAPS</td>
<td style="text-align:center">0.8938</td>
<td style="text-align:right">0.9797</td>
<td style="text-align:right">0.8259</td>
</tr>
</tbody>
</table>
<p>音频和视频初步融合</p>
<table>
<thead>
<tr>
<th>dataset</th>
<th style="text-align:center">F1-measure</th>
<th style="text-align:right">Precision</th>
<th style="text-align:right">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>OMAP</td>
<td style="text-align:center">0.7651</td>
<td style="text-align:right">0.9665</td>
<td style="text-align:right">0.6405</td>
</tr>
<tr>
<td>OMAPS</td>
<td style="text-align:center">0.8389</td>
<td style="text-align:right">0.9860</td>
<td style="text-align:right">0.7352</td>
</tr>
</tbody>
</table>
<hr>
<p>统计数据（在OMAP数据集根据纯音频模型的转录结果进行相应统计）</p>
<ol>
<li>各个琴键被按下的概率<br>
<img src="https://daydreamer-li.github.io//post-images/1663902300340.png" alt="" loading="lazy"><br>
可以看到我们的数据集中，按键频率呈高斯分布，其中白键按下概率高于黑键</li>
<li>纯音频下的多检漏检统计<br>
漏检统计<br>
<img src="https://daydreamer-li.github.io//post-images/1663917459588.png" alt="" loading="lazy"><br>
可以看到越靠近中间的漏检越多，尤以40最为突出，59、33也与琴键按下概率不一致，目前暂未发现特殊规律。<br>
多检统计<br>
<img src="https://daydreamer-li.github.io//post-images/1663917455009.png" alt="" loading="lazy"><br>
多检个数整体较少，没有可比性</li>
</ol>
<hr>
<h2 id="采用概率平均融合策略尝试即给视频的分支相应的少一点权重">采用概率平均融合策略尝试（即给视频的分支相应的少一点权重）</h2>
<p>为避免过拟合的模型在OMAP上有不好效果，已经对各个训练过程中f1较高的几个模型进行挑选<br>
模型地址：&quot;/home/data/liyuqing/code_VAF/onsets-and-frames-master/runs/loss_with_video_label-220926-203911/model-0.9207.pt&quot;</p>
<table>
<thead>
<tr>
<th>dataset</th>
<th style="text-align:center">F1-measure</th>
<th style="text-align:right">Precision</th>
<th style="text-align:right">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>OMAP</td>
<td style="text-align:center">0.8559</td>
<td style="text-align:right">0.9191</td>
<td style="text-align:right">0.8042</td>
</tr>
<tr>
<td>OMAPS</td>
<td style="text-align:center">0.9065</td>
<td style="text-align:right">0.9365</td>
<td style="text-align:right">0.8801</td>
</tr>
</tbody>
</table>
<p>通过按键频率对loss函数给予相应权重</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[self-attention网络搭建]]></title>
        <id>https://daydreamer-li.github.io/post/self-attention-code-by-myself/</id>
        <link href="https://daydreamer-li.github.io/post/self-attention-code-by-myself/">
        </link>
        <updated>2022-07-29T08:32:20.000Z</updated>
        <summary type="html"><![CDATA[<p>关于自注意力机制在钢琴演奏视频数据集上的实现</p>
]]></summary>
        <content type="html"><![CDATA[<p>关于自注意力机制在钢琴演奏视频数据集上的实现</p>
<!-- more -->
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[注意力机制、自注意力机制和Transformer系统学习]]></title>
        <id>https://daydreamer-li.github.io/post/attention_learning/</id>
        <link href="https://daydreamer-li.github.io/post/attention_learning/">
        </link>
        <updated>2022-07-27T09:05:21.000Z</updated>
        <summary type="html"><![CDATA[<p>关于注意力机制、自注意力机制和Transformer的学习</p>
]]></summary>
        <content type="html"><![CDATA[<p>关于注意力机制、自注意力机制和Transformer的学习</p>
<!-- more -->
<h1 id="attention机制2014年提出">Attention机制（2014年提出）</h1>
<p>相关论文：Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014</p>
<h2 id="1-引入attetion机制的原因">1. 引入attetion机制的原因</h2>
<p>（1）计算能力限制：信息越多，模型越复杂，计算能力限制神经网络的发展<br>
（2）优化算法限制：LSTM只能在一定程度上缓解RNN中的长距离依赖问题，且信息“记忆”能力并不高。<br>
（3） 缺点：需要的数据量大，当数据量少时，注意力机制效果不如BiLstm和LSTM</p>
<h2 id="2-简要理解和过程">2. 简要理解和过程</h2>
<p>（1） 定性解释： Attention是从大量信息中有筛选出少量重要信息，并聚焦到这些重要信息上，忽略大多不重要的信息。权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。<br>
（2） 抽象计算过程：第一个过程是根据Query和Key计算权重系数（根据Query和Key计算两者的相似性或者相关性），第二个过程根据权重系数对Value进行加权求和。</p>
<figure data-type="image" tabindex="1"><img src="https://daydreamer-li.github.io//post-images/1658976639132.jpg" alt="" title="attention计算示意图" loading="lazy"></figure>
<h2 id="3-具体过程">3.  具体过程：</h2>
<ul>
<li>第一阶段音符不同的函数和计算机制，根据Query和某一个keyi，计算两者相似性或者相关性，常见方法有：求两者的向量点积、求两者的向量Cosine相似性或者在引入额外的神经网络（例如MLP）求值；</li>
<li>第二阶段通过类似Softmax的方法对第一阶段得到的相关性值归一化结果，将原始的相关值通过函数转换为0-1的概率分布；</li>
<li>最后通过加权求和得到Attention的数值，具体公式如下，其中ai是第二阶段计算的概率值，为valuei对应的权重系数<br>
<img src="https://daydreamer-li.github.io//post-images/1658977491596.png" alt="" title="attention数值计算公式" loading="lazy"></li>
</ul>
<h1 id="self-attention机制">Self-attention机制</h1>
<p>原文链接：<a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p>
<h2 id="1-提升点和优缺点">1. 提升点和优缺点</h2>
<p>（1）相较于注意力机制而言，减少了对外部信息的依赖，更擅长捕捉数据或特征的内部相关性<br>
（2）优点<br>
- 参数少：相比于 CNN、RNN ，其复杂度更小，参数也更少。所以对算力的要求也就更小。<br>
- 速度快：Attention 解决了 RNN及其变体模型 不能并行计算的问题。Attention机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理。<br>
- 效果好：引入Attention 机制，可以学习长距离的信息<br>
（3）缺点同attention机制</p>
<h2 id="2-在文本中的计算过程通过计算单词间的相互影响来解决长距离依赖问题">2. 在文本中的计算过程（通过计算单词间的相互影响，来解决长距离依赖问题）</h2>
<p>（1） 将输入单词转化成嵌入向量；<br>
（2）根据嵌入向量得到q，k，v三个向量；<br>
（3）为每个向量计算一个score：score =q . k ；<br>
（4）为了梯度的稳定，Transformer使用了score归一化，即除以  ；<br>
（5）对score施以softmax激活函数；<br>
（6）softmax点乘Value值v，得到加权的每个输入向量的评分v；<br>
（7） 相加之后得到最终的输出结果z ：对 v求和。</p>
<h2 id="3-详细的self-attention处理过程">3. 详细的self-attention处理过程</h2>
<p>（1）首先，self-attention会计算出三个新的向量，在论文中，向量的维度是<strong>512维</strong>，我们把这三个向量分别称为Query、Key、Value，这三个向量是用embedding向量与一个矩阵相乘得到的结果，这个矩阵是随机初始化的，维度为（64，512）注意第二个维度需要和embedding的维度一样，其值在BP的过程中会一直进行更新，得到的这三个向量的维度是64低于embedding维度的<br>
<img src="https://daydreamer-li.github.io//post-images/1658991135377.png" alt="" loading="lazy"></p>
<p>接下来阐述Query、Key和value的具体作用：<br>
- 计算self-attention的分数值，该分数值决定了当我们在某个位置encode一个词时，对输入句子的其他部分的关注程度。这个分数值的计算方法是Query与Key做点乘，以下图为例，首先我们需要针对Thinking这个词，计算出其他词对于该词的一个分数值，首先是针对于自己本身即q1·k1，然后是针对于第二个词即q1·k2<br>
<img src="https://daydreamer-li.github.io//post-images/1658991687173.png" alt="" title="计算self-attention的分数值" loading="lazy"></p>
<ul>
<li>
<p>接下来，把点乘的结果除以一个常数，这里我们除以8，这个值一般是采用上文提到的矩阵的第一个维度的开方即64的开方8，当然也可以选择其他的值，然后把得到的结果做一个softmax的计算。得到的结果即是每个词对于当前位置的词的相关性大小。<br>
<img src="https://daydreamer-li.github.io//post-images/1658992021226.png" alt="" loading="lazy"></p>
<ul>
<li>下一步就是把Value和softmax得到的值进行相乘，并相加，得到的结果即是self-attetion在当前节点的值<a name="self-attention计算图"></a><br>
<img src="https://daydreamer-li.github.io//post-images/1658992081871.png" alt="" loading="lazy"><br>
<img src="https://daydreamer-li.github.io//post-images/1658992143076.png" alt="" loading="lazy"></li>
</ul>
</li>
</ul>
<p>这种通过query和key的相似程度来确定value的权重分布的方法被称为scaled dot-product attention<img src="https://daydreamer-li.github.io//post-images/1658993118948.png" alt="" title="矩阵计算公式" loading="lazy"></p>
<h1 id="transformer">Transformer</h1>
<p>** 总体概括：Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。更准确地讲，Transformer由且仅由self-Attenion和Feed Forward Neural Network组成。**<br>
原文链接：<a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a><br>
知乎链接：<a href="https://zhuanlan.zhihu.com/p/48508221">https://zhuanlan.zhihu.com/p/48508221</a><br>
##　1. 优点：<br>
（1）虽然Transformer最终也没有逃脱传统学习的套路，Transformer也只是一个全连接（或者是一维卷积）加Attention的结合体。但是其设计已经足够有创新，因为其抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩，值得每个深度学习的相关人员仔细研究和品位。（2）Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离是1，这对解决NLP中棘手的长期依赖问题是非常有效的。<br>
（3）Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向。<br>
（4）算法的并行性非常好，符合目前的硬件（主要指GPU）环境。</p>
<p>##　2. 缺点：<br>
（1）粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型丧失了捕捉局部特征的能力，RNN + CNN + Transformer的结合可能会带来更好的效果。<br>
（2）Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。</p>
<p>##　3. 高层Transformer简单结构<br>
Transformer本质上是一个Encoder-Decoder的结构，举例来说，当编码器由6个编码bloack组成，同样解码器是6个解码block组成，编码器的输出会作为所有解码block的输入。结构示意图如下：</p>
<figure data-type="image" tabindex="2"><img src="https://daydreamer-li.github.io//post-images/1659009668701.png" alt="" title="Encoder和Decoder的结构示意图" loading="lazy"></figure>
<ul>
<li>在Encoder中，数据首先会经过一个叫做‘self-attention’的模块得到一个加权之后的特征向量称为Z，具体公式见上文self-attention中最后一张图片；得到Z后，会被送到encoder的下一个模块，即Feed Forward NeuralNetwork，这个全连接层有两层，第一层的激活函数是ReLU，第二层是一个线性激活函数，可以表示为：<br>
<img src="https://daydreamer-li.github.io//post-images/1659010093449.png" alt="" title="Feed Forward Neural Network" loading="lazy"></li>
<li>Decoder的结构，它和encoder的不同之处在于Decoder多了一个Encoder-Decoder Attention，两个Attention分别用于计算输入和输出的权值：<br>
Self-Attention：当前翻译和已经翻译的前文之间的关系；<br>
Encoder-Decnoder Attention：当前翻译和编码的特征向量之间的关系。</li>
</ul>
<p>在self-attention中最后一点是采用了残差网络中的short-cut结构，解决深度学习中的退化问题，结果如下图所示:<br>
<img src="https://daydreamer-li.github.io//post-images/1658995016644.png" alt="" title="Self-attention中的short-cut连接" loading="lazy"></p>
<p>##　4. multi-head Attention<br>
Multi-Head Attention相当于<em>ｈ</em>个不同的self-attention的集成（ensemble），在这里我们以  <em>ｈ＝８</em>　举例说明。Multi-Head Attention的输出分成3步：<br>
<img src="https://daydreamer-li.github.io//post-images/1659010935776.png" alt="" loading="lazy"><br>
整个过程如下图，同self-attention一样，multi-head attention也加入了short-cut机制。<br>
<img src="https://daydreamer-li.github.io//post-images/1659010940540.png" alt="" loading="lazy"></p>
<h2 id="5encoder-decoder-attention">5.Encoder-Decoder Attention</h2>
<p>在解码器中，Transformer block比编码器中多了个encoder-cecoder attention。在encoder-decoder attention中，<em>Q</em>来自于解码器的上一个输出,<em>K</em>和<em>V</em>则来自于与编码器的输出。其计算方式完全和<a href="#self-attention%E8%AE%A1%E7%AE%97%E5%9B%BE">该图</a> 的过程相同。</p>
<p>由于在机器翻译中，解码过程是一个顺序操作的过程，也就是当解码第 <em>k</em>个特征向量时，我们只能看到第<em>k-1</em>及其之前的解码结果，论文中把这种情况下的multi-head attention叫做masked multi-head attention。</p>
<h2 id="6损失层">6.损失层</h2>
<p>解码器解码之后，解码的特征向量经过一层激活函数为softmax的全连接层之后得到反映每个单词概率的输出向量。此时我们便可以通过CTC等损失函数训练模型了。</p>
<p>而一个完整可训练的网络结构便是encoder和decoder的堆叠（各<em>N</em>个，<em>N=6</em>），我们可以得到下图中的完整的Transformer的结构：<img src="https://daydreamer-li.github.io//post-images/1659011461553.png" alt="" loading="lazy"></p>
<h2 id="7位置编码">7.位置编码</h2>
<p>截止目前为止，我们介绍的Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子的结构怎么打乱，Transformer都会得到类似的结果。换句话说，Transformer只是一个功能更强大的词袋模型而已。</p>
<p>为了解决这个问题，论文中在编码词向量时引入了位置编码（Position Embedding）的特征。具体地说，位置编码会在词向量中加入了单词的位置信息，这样Transformer就能区分不同位置的单词了。</p>
<p>那么怎么编码这个位置信息呢？常见的模式有：a. 根据数据学习；b. 自己设计编码规则。在这里作者采用了第二种方式。那么这个位置编码该是什么样子呢？通常位置编码是一个长度为<em>dmodel</em>的特征向量，这样便于和词向量进行单位加的操作<br>
<img src="https://daydreamer-li.github.io//post-images/1659011529361.png" alt="" title="位置编码公式" loading="lazy"><br>
<img src="https://daydreamer-li.github.io//post-images/1659011550064.png" alt="" title="位置编码公式解释" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[合并数据集之后的结果记录]]></title>
        <id>https://daydreamer-li.github.io/post/dataset_and_result/</id>
        <link href="https://daydreamer-li.github.io/post/dataset_and_result/">
        </link>
        <updated>2022-07-22T03:10:34.000Z</updated>
        <summary type="html"><![CDATA[<p>包含OMAP\OMAPS合并数据集、MIDITest、PianoYT和Two Hands Hanon</p>
]]></summary>
        <content type="html"><![CDATA[<p>包含OMAP\OMAPS合并数据集、MIDITest、PianoYT和Two Hands Hanon</p>
<!-- more -->
<p>首先对各个数据集进行介绍：<br>
<strong>OMAP/OMAPS合并数据集</strong>：共206首，其中训练集146，验证集20，测试集40首<br>
<strong>midiTest</strong>：8个业余钢琴家录制的视频，仅含视频<br>
<strong>PianoYT</strong>: youtube的钢琴录制视频，共20多个小时，将数据分为209个训练验证，19个测试集<br>
<strong>Two Hands Hanon</strong>：该数据集包含的和弦较少，挑战性较小，音符比MIDI测试集中的音符范围更小</p>
<blockquote>
<p>PianoYT需要自行在Youtube上下载视频并与谷歌的音频数据集对应；Two Hands Hanon未开源？</p>
</blockquote>
<p><strong>没有进行数据增强</strong><br>
帧级精度</p>
<table>
<thead>
<tr>
<th>dataset</th>
<th style="text-align:center">F1-measure</th>
<th style="text-align:right">Precision</th>
<th style="text-align:right">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>fusion</td>
<td style="text-align:center">0.7627</td>
<td style="text-align:right">0.8809</td>
<td style="text-align:right">0.7013</td>
</tr>
<tr>
<td>midiTest</td>
<td style="text-align:center">0.1644</td>
<td style="text-align:right">0.2658</td>
<td style="text-align:right">0.1327</td>
</tr>
<tr>
<td>PianoYT</td>
<td style="text-align:center">暂无</td>
<td style="text-align:right">暂无</td>
<td style="text-align:right">暂无</td>
</tr>
</tbody>
</table>
<blockquote>
<p>数据说明：recall高漏检少，precision高多检少；即p低多检多，r低漏检多</p>
</blockquote>
<hr>
<p>音符级精度</p>
<table>
<thead>
<tr>
<th>dataset</th>
<th style="text-align:center">F1-measure</th>
<th style="text-align:right">Precision</th>
<th style="text-align:right">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>fusion</td>
<td style="text-align:center">0.8828</td>
<td style="text-align:right">0.9222</td>
<td style="text-align:right">0.8511</td>
</tr>
<tr>
<td>midiTest</td>
<td style="text-align:center">0.012</td>
<td style="text-align:right">0.047</td>
<td style="text-align:right">0.010</td>
</tr>
<tr>
<td>PianoYT</td>
<td style="text-align:center">暂无</td>
<td style="text-align:right">暂无</td>
<td style="text-align:right">暂无</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>进行了数据增强</strong><br>
（测试集F1=0.797）<br>
帧级精度</p>
<table>
<thead>
<tr>
<th>dataset</th>
<th style="text-align:center">F1-measure</th>
<th style="text-align:right">Precision</th>
<th style="text-align:right">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>fusion（F1=0.729）</td>
<td style="text-align:center">0.7295</td>
<td style="text-align:right">0.6379</td>
<td style="text-align:right">0.8517</td>
</tr>
<tr>
<td>fusion（F1=0.797）</td>
<td style="text-align:center">0.7198</td>
<td style="text-align:right">0.7858</td>
<td style="text-align:right">0.6972</td>
</tr>
<tr>
<td>midiTest（F1= 0.729）</td>
<td style="text-align:center">0.2128</td>
<td style="text-align:right">0.3269</td>
<td style="text-align:right">0.1961</td>
</tr>
<tr>
<td>midiTest（F1= 0.797）</td>
<td style="text-align:center">0.1841</td>
<td style="text-align:right">0.3014</td>
<td style="text-align:right">0.1543</td>
</tr>
<tr>
<td>col 3 is</td>
<td style="text-align:center">right-aligned</td>
<td style="text-align:right">$1</td>
<td style="text-align:right"></td>
</tr>
</tbody>
</table>
<hr>
<p>音符级精度</p>
<table>
<thead>
<tr>
<th>dataset</th>
<th style="text-align:center">F1-measure</th>
<th style="text-align:right">Precision</th>
<th style="text-align:right">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>fusion（F1=0.729）</td>
<td style="text-align:center">0.7640</td>
<td style="text-align:right">0.7489</td>
<td style="text-align:right">0.7874</td>
</tr>
<tr>
<td>fusion（F1=0.797）</td>
<td style="text-align:center">0.7198</td>
<td style="text-align:right">0.7858</td>
<td style="text-align:right">0.6972</td>
</tr>
<tr>
<td>midiTest（F1= 0.797）</td>
<td style="text-align:center">0.0057</td>
<td style="text-align:right">0.0067</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td>col 3 is</td>
<td style="text-align:center">right-aligned</td>
<td style="text-align:right">$1</td>
<td style="text-align:right"></td>
</tr>
</tbody>
</table>
<p>conclusion:合并数据集和MIDItest中图片有较大差异，为了保留关键点和骨骼的信息，向下截取了50个像素点，而整张图送进去没有使用注意力信息，无法将网络训练关注在手指上，同时midiTest是固定图的大小，为提升midiTest精度可以把数据集上的图片进行这样的处理，但个人觉得没有必要。<br>
<img src="https://daydreamer-li.github.io//post-images/1658481473351.jpg" alt="" title="midiTest示意图" loading="lazy"><br>
<img src="https://daydreamer-li.github.io//post-images/1658481480183.jpg" alt="" title="OMAPS示意图" loading="lazy"><br>
<img src="https://daydreamer-li.github.io//post-images/1658481484012.jpg" alt="" title="OMAP示意图" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[一些论文研读]]></title>
        <id>https://daydreamer-li.github.io/post/thesis_study/</id>
        <link href="https://daydreamer-li.github.io/post/thesis_study/">
        </link>
        <updated>2022-07-20T07:10:49.000Z</updated>
        <summary type="html"><![CDATA[<p>包括F-T-LSTM、Uformer和视听多模态相关的论文学习</p>
]]></summary>
        <content type="html"><![CDATA[<p>包括F-T-LSTM、Uformer和视听多模态相关的论文学习</p>
<!-- more -->
<h1 id="f-t-lstm">F-T-LSTM</h1>
<h2 id="1-基本情况介绍">1. 基本情况介绍</h2>
<p><strong>论文标题</strong>：F-T-LSTM based Complex Network for Joint Acoustic Echo   Cancellation and Speech Enhancement<br>
<strong>From</strong>：西北工业大学计算机学院语音与语言处理组（ASLP@NPU)<br>
<strong>项目地址</strong>：暂无<br>
<strong>创新点</strong>：提出一种使用复杂神经网络的实时 AEC 方法，以更好地建模重要的相位信息和频率时间 LSTM (F-T-LSTM)，通过扫描频率和时间轴的方式，以实现更好的时间建模<br>
<strong>关键词</strong>： Acoustic echo cancellation（AEC回声抵消）, complex network （复杂网络），nonlinear distortion（非线性失真）, noise suppression（噪声抑制）</p>
<h2 id="2-论文核心内容">2. 论文核心内容</h2>
<hr>
<h1 id="uformer原理">Uformer原理</h1>
<h2 id="1-基本情况介绍-2">1. 基本情况介绍</h2>
<p><strong>论文标题</strong>:  Uformer: A General U-Shaped Transformer for Image Restoration<br>
<strong>github项目地址</strong>: <a href="https://github.com/ZhendongWang6/Uformer">https://github.com/ZhendongWang6/Uformer</a></p>
<hr>
<h1 id="uformer可借用融合">Uformer可借用融合</h1>
<p><strong>论文标题</strong>：UFORMER: A UNET BASED DILATED COMPLEX &amp; REAL DUAL-PATH CONFORMER NETWORK FOR SIMULTANEOUS SPEECH ENHANCEMENT AND DEREVERBERATION</p>
<hr>
<h1 id="音视频融合by-indian">音视频融合By Indian</h1>
<h2 id="1-基本情况介绍-3">1. 基本情况介绍</h2>
<p><strong>论文标题</strong>：WHAT MAKES THE SOUND?: A DUAL-MODALITY INTERACTING NETWORK FOR AUDIO-VISUAL EVENT LOCALIZATION<br>
<strong>创新点</strong>:  提出了一种新的视听交互网络（AVIN），通过两种模态间的局部和全局信息，实现了模态间和模态内的交互</p>
<h2 id="2-论文内容详读">2. 论文内容详读</h2>
<figure data-type="image" tabindex="1"><img src="https://daydreamer-li.github.io//post-images/1658305526152.png" alt="网络结构示意图" title="网络结构示意图" loading="lazy"></figure>
<blockquote>
<p>一共包含六个步骤，</p>
<ol>
<li>使用CNN进行特征提取</li>
<li>再使用LSTMs建模时间依赖性</li>
<li>使用双线性池捕获两种模式之间的高层关联</li>
<li>使用两种模态的局部和全局信息获得内部和内部模态交互<br>
5.聚合所有的结果特征，通过dense layer传递连接输出</li>
</ol>
</blockquote>
]]></content>
    </entry>
</feed>